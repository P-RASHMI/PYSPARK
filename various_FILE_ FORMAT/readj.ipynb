{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession,types\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- RecordNumber: long (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- Zipcode: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''To read from json file from hdfs'''\n",
    "df = spark.read.json(\"hdfs://localhost:9000/jsonread/readexample.json\",multiLine=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----+-----------+-------+\n",
      "|               City|RecordNumber|State|ZipCodeType|Zipcode|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "|PASEO COSTA DEL SUR|           2|   PR|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|          10|   PR|   STANDARD|    709|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "'''To write to the json file in hdfs'''\n",
    "df.write.json(\"hdfs://localhost:9000/jsonread/writeto.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
      "|survived|pclass|   sex| age|sibsp|parch|   fare|embarked|class|  who|adult_male|deck|embark_town|alive|alone|\n",
      "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|Third|  man|      True|null|Southampton|   no|False|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|First|woman|     False|   C|  Cherbourg|  yes|False|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|Third|woman|     False|null|Southampton|  yes| True|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       S|First|woman|     False|   C|Southampton|  yes|False|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       S|Third|  man|      True|null|Southampton|   no| True|\n",
      "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''to read csv file from hdfs'''\n",
    "df1 = spark.read.csv(\"hdfs://localhost:9000/csvspark/titanic.csv\",header=True)\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "'''Write to csv file in hdfs'''\n",
    "df1.write.option(\"header\",\"true\")\\\n",
    ".csv(\"hdfs://localhost:9000/jsonread/writeto.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''To create dataframe from that write to parquet file in hdfs and \n",
    "# from that read the parquet file data from hdfs'''\n",
    "# # import pandas as pd\n",
    "# data =[(\"Hary \",\"\",\"mehak\",\"M\",\"3000\"),\n",
    "#               (\"Michael \",\"vanith\",\"\",\"M\",\"4000\"),\n",
    "#               (\"Rithika \",\"\",\"devi\",\"F\",\"4000\"),\n",
    "#               (\"Maria \",\"Anne\",\"Jones\",\"F\",\"4000\"),\n",
    "#               (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",\"12000\")]\n",
    "# columns=[\"firstname\",\"middlename\",\"lastname\",\"gender\",\"salary\"]\n",
    "# df_parquet=spark.createDataFrame(data,columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_parquet.write.parquet(\"hdfs://localhost:9000/jsonread/writeto.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession,types\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "'''To read the csv file from hdfs and convert to parquet file'''\n",
    "\n",
    "dataframe_csv = spark.read.csv(\"hdfs://localhost:9000/csvspark/titanic.csv\",header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "dataframe_csv.write.parquet(\"hdfs://localhost:9000/jsonread/writeto.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
      "|survived|pclass|   sex| age|sibsp|parch|   fare|embarked|class|  who|adult_male|deck|embark_town|alive|alone|\n",
      "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|Third|  man|      True|null|Southampton|   no|False|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|First|woman|     False|   C|  Cherbourg|  yes|False|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|Third|woman|     False|null|Southampton|  yes| True|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       S|First|woman|     False|   C|Southampton|  yes|False|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       S|Third|  man|      True|null|Southampton|   no| True|\n",
      "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "'''To read the parquet file from hdfs'''\n",
    "parquetread = spark.read.parquet(\"hdfs://localhost:9000/jsonread/writeto.parquet\")\n",
    "parquetread.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''To convert to avro file,read the data from csv and convert to avro'''\n",
    "\n",
    "# dataframe_csv.write.format(\"avro\").save(\"hdfs://localhost:9000/jsonread/writeto.avro\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
